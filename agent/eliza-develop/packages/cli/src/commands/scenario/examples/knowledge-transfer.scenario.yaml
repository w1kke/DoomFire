name: 'Knowledge Transfer Effectiveness Test'
description: "Test agent's ability to teach complex concepts through multi-turn conversation"

plugins:
  - name: '@elizaos/plugin-bootstrap'
    enabled: true
  - name: '@elizaos/plugin-sql'
    enabled: true
  - name: '@elizaos/plugin-openai'
    enabled: true

environment:
  type: local

run:
  - name: 'Teaching machine learning concepts to beginner'
    input: "Can you explain how machine learning works? I'm not very technical but I'm curious"

    conversation:
      max_turns: 10
      timeout_per_turn_ms: 35000

      user_simulator:
        model_type: 'TEXT_LARGE'
        temperature: 0.7
        max_tokens: 250
        persona: 'curious non-technical professional who wants to understand ML basics'
        objective: 'gain practical understanding of machine learning concepts and applications'
        style: 'eager to learn but needs simple explanations and concrete examples'
        constraints:
          - 'Ask clarifying questions when concepts seem abstract'
          - 'Request simpler explanations for complex technical terms'
          - "Give feedback on understanding level ('I think I get it' or 'I'm still confused')"
          - 'Ask for practical, real-world examples'
          - 'Show appreciation for clear explanations with analogies'
          - 'Ask follow-up questions about applications in their field'
        emotional_state: 'curious and motivated but slightly intimidated by technical topics'
        knowledge_level: 'beginner'

      termination_conditions:
        - type: 'user_expresses_satisfaction'
          keywords: ['I understand now', 'that makes sense', 'clear explanation', 'thank you']
        - type: 'custom_condition'
          llm_judge:
            prompt: 'Does the user demonstrate a solid basic understanding of machine learning concepts?'
            threshold: 0.8

      turn_evaluations:
        - type: 'llm_judge'
          prompt: "Was the explanation appropriate for a beginner's level?"
          expected: 'yes'
        - type: 'llm_judge'
          prompt: 'Did the agent use helpful analogies or concrete examples?'
          expected: 'yes'
        - type: 'llm_judge'
          prompt: 'Did the agent check for understanding before moving to next concept?'
          expected: 'yes'

      final_evaluations:
        - type: 'llm_judge'
          prompt: 'Did the agent successfully teach machine learning concepts to a non-technical user?'
          expected: 'yes'
          capabilities:
            - 'Adapted language and explanations to beginner level'
            - 'Used concrete examples and analogies to explain abstract concepts'
            - 'Checked understanding regularly throughout conversation'
            - 'Built concepts progressively from simple to more complex'
            - 'Encouraged questions and created safe learning environment'
            - "Provided practical applications relevant to user's interests"
        - type: 'conversation_length'
          min_turns: 6
          max_turns: 12
          optimal_turns: 8
        - type: 'conversation_flow'
          required_patterns:
            - 'question_then_answer'
            - 'clarification_cycle'
          flow_quality_threshold: 0.8
        - type: 'user_satisfaction'
          satisfaction_threshold: 0.8
          measurement_method: 'llm_judge'
        - type: 'context_retention'
          test_memory_of:
            ['machine learning', 'beginner level', 'non-technical', 'practical examples']
          retention_turns: 6
          memory_accuracy_threshold: 0.85

    evaluations:
      - type: 'string_contains'
        value: 'machine learning'
      - type: 'llm_judge'
        prompt: 'Was the explanation educational and appropriate for the audience?'
        expected: 'yes'

judgment:
  strategy: all_pass
